{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Cleaning_function.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"puYCZ8ONdllc","colab_type":"text"},"source":["# Cleanin algorithm for financial series data based on Barndorff‐Nielsen, Hansen et al. 2009 – Realized kernels in practice"]},{"cell_type":"code","metadata":{"id":"jiamA_EMdllk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597241961728,"user_tz":-120,"elapsed":1113,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["import pandas as pd\n","import numpy as np\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vk-z3PvHdlmM","colab_type":"text"},"source":["### Pre processing"]},{"cell_type":"code","metadata":{"id":"mL9rfzAKdlmb","colab_type":"code","colab":{}},"source":["data = data[data.notna()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AyFAKrTbdlmz","colab_type":"text"},"source":["### Cleaning general data (Trades and Quotes)"]},{"cell_type":"code","metadata":{"id":"4UXUh1Tbdlm7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597241971416,"user_tz":-120,"elapsed":965,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["def clean_0(serie, origin = 0, trade = False):\n","    \n","    \"\"\"Function to clean trade or quote data from WRDS-Wharton TAQ data set, based on stepts \"P1\" to \"P3\" \n","     in Barndorff‐Nielsen, Hansen et al. 2009 \n","    \n","    Parameters\n","    ----------\n","    serie: Dataframe\n","           Data frame containing all the trade or quote data in the time span 9:30 to 16:00 \n","        \n","       \n","    origin: String\n","       Select the origin of the trade (e.g. N for NYSE). By default the function takes all the origins\n","       Exchange on which the trade occurred:\n","        A – AMEX\n","        N – NYSE\n","        B – Boston\n","        P – Arca\n","        C – NSX\n","        T/Q – NASDAQ\n","        D – NASD ADF and TRF\n","        X – Philadelphia\n","        I – ISE\n","        M – Chicago\n","        W – CBOE\n","        Z – BATS\n","        1 – Nasdaq prints in Nasdaq stocks Aug/Sep 2006 only\n","\n","    trade = Boolean\n","        False if the data set corresponds to quote data. True if it is trade data\n","       \n","    Returns\n","    -------\n","    Dataframe\n","        Basic cleaned data set\n","        \n","        \"\"\"\n","    \n","    \n","    #P1. Drop observations before 9:30 and after 16:00\n","    #Already done\n","    \n","    #P2. Delete entries with a bid, ask or transaction price equal to zero \n","        \n","    if trade == False:\n","        clean_data0 = serie[(serie.BID != 0) & (serie.ASK != 0)]\n","    else:\n","        clean_data0 = serie[serie.PRICE != 0]\n","\n","    #P3. Retain entries originating from a single exchange (NYSE in our application). Delete other entries\n","    \n","    if origin != 0:\n","        clean_data0 = clean_data0[clean_data0.EX == origin] \n","    else:\n","        clean_data0 = clean_data0\n","\n","    return clean_data0     \n","        \n","    "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vdunEj7dlnQ","colab_type":"text"},"source":["### Trade Data Only"]},{"cell_type":"code","metadata":{"id":"tkcbBM5NdlnU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597243824139,"user_tz":-120,"elapsed":880,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["def clean_trade(clean_data):\n","    \"\"\"Function to clean trade data from WRDS-Wharton TAQ data set, based in stepts \"T1\" to \"T3\" \n","     in Barndorff‐Nielsen, Hansen et al. 2009\n","    \n","    Parameters\n","    ----------\n","    clean_data: Dataframe\n","           Data frame containing data generated by clean_0 function\n","           \n","    Returns\n","    -------\n","    Dataframe\n","        Cleaned set for trade data\n","        \n","    \"\"\"\n","    #T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0)\n","    \n","    clean_data = clean_data[clean_data.TR_CORR == 0]\n","    \n","    #T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for ‘E’ and ‘F’).\n","    \n","    clean_data[\"TR_SCOND\"].fillna(\"@\", inplace = True)\n","    \n","    clean_data = clean_data[(clean_data.TR_SCOND == '@')| \n","                            (clean_data.TR_SCOND == \"@E\")|\n","                            (clean_data.TR_SCOND == \"E\")|\n","                            (clean_data.TR_SCOND == \"F\")| \n","                            (clean_data.TR_SCOND == \"@F\")]\n","    \n","    #clean_data.TR_SCOND.value_counts() # if we want to know how many of each category are in the sample\n","    time = clean_data.DATE.apply(str) + clean_data.TIME_M\n","    clean_data['TIME_M'] = pd.to_datetime(time,format= \"%Y%m%d%H:%M:%S.%f\")\n","    \n","    #T3. If multiple transactions have the same time stamp, use the median price.\n","    clean_data=clean_data.groupby(['TIME_M'], as_index=False).agg({'DATE':'first',\n","                                                      'SYM_ROOT':'first',\n","                                                      'TR_SCOND':'first', \n","                                                      'SIZE':'first',\n","                                                      'PRICE':'median'}).sort_values(by=['SYM_ROOT','TIME_M']).reset_index(drop=True)\n","    \n","    #T4, is replaced by Q4 since the time stamp of quotes and trades does not coincide\n","    \"\"\"Q4 Delete entries for which the mid-quote deviated by more than 10 mean absolute \n","    deviations from a rolling centred median (excluding the observation under consideration) \n","    of 50 observations (25 observations before and 25 after)\"\"\"\n","    \n","    price = np.array(clean_data.PRICE)\n","    med = []\n","    m_a_d = []\n","    for i in range(0, len(price)):\n","        p = np.concatenate((price[i-25:i],price[i+1:i+26]), axis=0)\n","        med_0 = np.median(p)\n","        med.append(med_0)\n","        m_a_d.append(np.mean(np.abs(p - med_0)))\n","    \n","    m_a_d = np.array(m_a_d)\n","    med = np.array(med)\n","    clean_data = clean_data.drop(clean_data[abs(clean_data.PRICE-med)>10*m_a_d].index)\n","    \n","    return clean_data\n"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjcqRmOtdpNa","colab_type":"text"},"source":["### Quote data only "]},{"cell_type":"code","metadata":{"id":"Dz2KHZNXdlnq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597241979387,"user_tz":-120,"elapsed":961,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["def clean_quote(clean_data):\n","    \"\"\"Function to clean quote data from WRDS-Wharton TAQ data set, based in stepts \"Q1\" to \"Q4\" \n","     in Barndorff‐Nielsen, Hansen et al. 2009\n","    \n","    Parameters\n","    ----------\n","    clean_data: Dataframe\n","           Data frame containing data generated by clean_0 function\n","           \n","    Returns\n","    -------\n","    Dataframe\n","        Cleaned set for quote data\n","        \n","    \"\"\"\n","    \n","    #Q1. When multiple quotes have the same time stamp, we replace all these with a single entry\n","        #with the median bid and median ask price.\n","    \n","    time = clean_data.DATE.apply(str) + clean_data.TIME_M\n","    #clean_data['TIME_M'] = pd.to_datetime(clean_data.TIME_M,format= \"%Y-%m%-d%H:%M:%S.%f\")\n","    clean_data['TIME_M'] = pd.to_datetime(time,format= \"%Y%m%d%H:%M:%S.%f\")\n","\n","    clean_data=clean_data.groupby(['TIME_M'], as_index=False).agg({'DATE':'first',\n","                                                          'SYM_ROOT':'first',\n","                                                          'QU_SOURCE':'first', \n","                                                          'BID':'median',\n","                                                          'ASK':'median',\n","                                                          'ASKSIZ':'sum',\n","                                                          'BIDSIZ':'sum',\n","                                                          }).sort_values(by=['SYM_ROOT','TIME_M']).reset_index(drop=True)\n","    \n","    #Q2. Delete entries for which the spread is negative.\n","    \n","    clean_data['SPREAD'] = clean_data.ASK - clean_data.BID\n","    \n","    clean_data = clean_data[(clean_data.SPREAD > 0)]\n","    \n","    #Q3 Delete entries for which the spread is more that 50 times the median spread on that day\n","    \n","    data_day = clean_data.groupby(['DATE'])['SPREAD'].median().reset_index()\n","    \n","    clean_data['MEDIAN_SPREAD'] = 0\n","    \n","    for i in range(len(data_day)):\n","        clean_data.MEDIAN_SPREAD.loc[clean_data.DATE == data_day.DATE[0]] = data_day.SPREAD[0]\n","    \n","    clean_data = clean_data[clean_data.SPREAD < 50 * clean_data.MEDIAN_SPREAD].copy()\n","    #Q4 Delete entries for which the mid-quote deviated by more than 10 mean absolute deviations from\n","    #a rolling centred median (excluding the observation under consideration) \n","    #of 50 observations (25 observations before and 25 after)\n","    \n","    clean_data['MID_QUOTE'] = 0.5 * (clean_data.ASK + clean_data.BID)\n","    price = np.array(clean_data.MID_QUOTE)\n","    med = []\n","    m_a_d = []\n","    for i in range(0, len(price)):\n","        p = np.concatenate((price[i-25:i],price[i+1:i+26]), axis=0)\n","        med_0 = np.median(p)\n","        med.append(med_0)\n","        m_a_d.append(np.mean(np.abs(p - med_0)))\n","    \n","    m_a_d = np.array(m_a_d)\n","    med = np.array(med)\n","    clean_data = clean_data.drop(clean_data[abs(clean_data.MID_QUOTE-med)>10*m_a_d].index)\n","    \n","    return clean_data\n","        "],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIa0S2ACe36h","colab_type":"text"},"source":["## Cleaning data"]},{"cell_type":"code","metadata":{"id":"pSbWQwwqe1MC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597242001416,"user_tz":-120,"elapsed":815,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["def cleaner(serie, origin = 0, trade = False):\n","  \"\"\"Function to clean quote or trade data from WRDS-Wharton TAQ data set, based in stepts \"Q1\" to \"Q4\" \n","     in Barndorff‐Nielsen, Hansen et al. 2009\n","    \n","    Parameters\n","    ----------\n","    serie: Dataframe\n","           Data frame containing data generated from WRDS-Wharton TAQ data set\n","           \n","    Returns\n","    -------\n","    Dataframe\n","        Cleaned set for quote or trade data\n","        \n","    \"\"\"\n","\n","  clean_data0 = clean_0(serie, origin, trade)\n","\n","  if trade == False:\n","    clean_data = clean_quote(clean_data0)\n","  else:\n","    clean_data = clean_trade(clean_data0)\n","\n","  return clean_data"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yysY-I-g1ut","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597242916853,"user_tz":-120,"elapsed":1070,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}},"outputId":"f5bc42b7-543e-4eef-ab83-2994c24171f8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D8rV94SWgZx8","colab_type":"code","colab":{}},"source":["# Dataset Quotes Apple 06.05.2010\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/AAPL Apple 06.05.2010 QUOTE.csv')\n","apple_clean = cleaner(data)\n","apple_clean.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/apple_cleaned_quote.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nl5zrx8bhrzc","colab_type":"code","colab":{}},"source":["# Dataset Quotes Dow Jones (DIA ETF of DJIA)\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/DIA Dow Jones 05.02.2018 QUOTE.csv')\n","dia_clean = cleaner(data)\n","dia_clean.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/djia_cleaned_quote.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ197FAhh7xf","colab_type":"code","colab":{}},"source":["#Dataset Quotes 3M 05.02.2018\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/MMM 3M 05.02.2018 QUOTE.csv')\n","mmm_clean = cleaner(data)\n","mmm_clean.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/mmm_cleaned_quote.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bEU6Qt5is3x","colab_type":"code","colab":{}},"source":["#Dataset Quotes S&P 500 (SPY DTF) 06.05.2010\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/SPY SP 500 06.05.2010 QUOTE.csv')\n","sp_clean_1 = cleaner(data)\n","sp_clean_1.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/s&p_cleaned_quote1.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxeRgyetm12r","colab_type":"code","colab":{}},"source":["#Dataset Quotes S&P 500 (SPY DTF) 24.08.2015\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/SPY SP 500 24.08.2015 QUOTE.csv')\n","sp_clean_2 = cleaner(data)\n","sp_clean_2.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/s&p_cleaned_quote2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdQU95BnDIPy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597243877118,"user_tz":-120,"elapsed":39,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["#Dataset trades S&P 500 (SPY DTF) 06.05.2010\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/SPY 06.05.2010 TRADE.csv')\n","sp_clean_trade = cleaner(data,trade=True)\n","sp_clean_trade.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/s&p_cleaned_trade.csv')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2nEmlpAamc3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597249129817,"user_tz":-120,"elapsed":28,"user":{"displayName":"Octavio Cardona","photoUrl":"","userId":"07900524146014769136"}}},"source":["#Dataset trades Dow Jones (DIA ETF of DJIA) 05.02.2018\n","data = pd.read_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/DIA Dow Jones 05.02.2018 TRADE.csv')\n","dia_clean_trade = cleaner(data,trade=True)\n","dia_clean_trade.to_csv('/content/drive/My Drive/Tesis Konstanz/Clean data/data/djia_cleaned_trade.csv')"],"execution_count":20,"outputs":[]}]}